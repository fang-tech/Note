## TCP连接断开

### TCP四次挥手的过程

![](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230614791.png)

- 客户端打算关闭连接, 客户端发送`FIN`报文, 进入`FIN_WAIT_1`状态
- 服务端接受到了`FIN_WAIT_1`报文, 进入`CLOSED_WAIT`状态, 回复`ACK`报文, 服务端接受到了`ACK`报文, 进入到`FIN_WAIT_2`状态
- 服务端发送`FIN`报文, 进入`LAST_ACK`状态
- 客户端接收到了`FIN`报文, 客户端发送`ACK`报文, 进入到`TIME_WAIT`状态
- 服务端在收到`ACK`报文以后, 进入到`CLOSE`状态
- 客户端在经过`2MSL`一段时间后, 自动进入到`CLOSE`状态

主动关闭连接的, 才有TIME_WAIT状态

### 为什么挥手需要四次而不是三次?

- 关闭连接的时候, 客户端向服务端发送FIN, 仅仅表示客户端不再发送数据了但是还能接收数据
- 服务端接收到客户端的FIN报文时, 先回一个ACK报文, 而服务端可能还有数据需要处理和发送, 等服务端不再发送数据的时候, 才发送FIN报文给客户端表示同意关闭现在的连接

但是也有三次, 在被动关闭方TCP挥手的过程中, 如果没有数据要发送, 并且没有开启TCP_QUICKACK(默认没有开启), 就会启用TCP延迟确认机制, 第二次和第三次挥手就会合并

> TCP延迟确认机制

- 当有响应数据要发送的时候, ACK会和响应数据一起发送给对象
- 当没有响应数据要发送的时候, ACK将会延迟一段时间, 等待是否有响应数据可以一起发送给对方
- 如果在延迟等待发送ACK期间, 对方的第二个数据段又到达了, 就会立即发送ACK

### 第一次挥手丢失会发生什么

主动方发送的`FIN`报文丢失, 主动方在发送报文以后就进入到了`FIN_WAIT_1`状态, 主动方会在超时时间内没有接收到ACK响应报文而触发超时重传

超时时间会随着超时次数翻倍

重发次数通过`tcp_orphan_retries`参数来控制

### 第二次挥手丢失会发生什么

被动方接收到了发起方的`FIN`报文, 进入到`CLOSE_WAIT`状态

但是ACK报文是不会重传的, 从发起方的角度来看, 就是自己的第一次挥手丢失了, 就会超时重传第一次挥手



### 第三次挥手丢失会发生什么

发起方接收到ACK报文以后, 进入到` FIN_WAIT_2`状态, 需要接收到被动方发送的`FIN`报文才能进入到`TIME_WAIT`状态

从客户端的角度来看就是一直没有接收到被动方发送的FIN报文, 一直在FIN_WAIT_状态, 从服务端的角度来看, 就是发送的FIN丢失了, 超时重传, 超过了设定的重试次数, 就会从服务端断开连接

如果主动关闭方使用的关闭函数是`close()`函数, 它有一个`tcp_fin_timeout`的参数, 控制了在`FIN_WAIT_2`阶段的最长时间, 如果超过了这个时间, TCP连接会被强行关闭, 从客户端断开丽娜姐

如果使用的是`shutdown()`函数, 那就是死等了

### 第四次挥手丢失会发生什么

客户端在第三次挥手以后就会进入到TIME_WAIT阶段, 2MSL之后就会从客户端关闭连接

- 服务端重传`FIN`报文的次数达到`tcp_orphan_retries`的最大重传次数的时候, 等待上次等待时间的两倍以后, 就会从服务端断开连接
- 客户端进入到`TIME_WAIT`阶段以后, 开启2MSL的定时器, 如果收到`FIN`报文, 就会重置定时器, 定时器到了, 就从客户端关闭连接

### 为什么TIME\_WAIT等待的时间是2MSL

`MSL` : Maximum Segment Lifetime, 报文的最大生存时间, 任何报文在网络上存在的最长时间

MSL与TTL的区别 : **MSL的单位是时间, TTL是经过路由跳数**

**TTL的值一般是64, Linux将MSL的值设置为30s, 意味着Linux认为数据经过64个路由器的时间不会超过30s, 如果超过了, 就认为报文已经消失在网络中了**

TIME_WAIT等待2倍的MSL, 比较合理的解释是 : 网络中可能存在发送方发送过来的数据包(也就是第三次挥手的FIN包), 接收到这个包的最长可能时间是一个MSL, 然后接收到以后又会给对方响应, 这个最大可接受时间又是一个MSL

相当于至少允许报文丢失一次, 如果ACK在一个MSL内丢失, 这样被动方重发的FIN会在第2个MSL内到达

### 为什么需要TIME_WAIT状态

TIME_WAIT是只有主动方才有的状态

- 防止历史连接中的数据, 被后面相同的四元组连接错误接收
  - 2MSL的时长能保证历史数据包都已经自然消失在网络中了
- 确保被动关闭连接的一方能正确关闭
  - 等待足够的时间,  保证自己的ACK是能被对方收到的, 从而帮助其自然关闭

### TIME_WAIT过多有什么危害

- 占用系统资源, 比如文件描述符, 内存资源, CPU资源, 线程资源等, 简单来说就是在TIME_WAIT过多会导致资源得不到快速的释放
- 占用端口资源, 端口资源是有限的, 一般范围是`32768 ~ 61000`, 可以通过修改net.ipv4.ip_local_port_range
  - 如果客户端(主动发起关闭连接方)TIME_WAIT状态过多, 就没办法再向[目的IP, 目的PORT]一样的服务建立连接了, 因为四元组相同

### 如何优化TIME_WAIT

优化的点在于TIME_WAIT会延缓资源释放的时间, 解决最短等待时间是`2MSL`的问题

*方式一 : net.ipv4.tcp_tw_reuse和tcp_timestamps*

开启`net.ipv4.tcp_te_reuse = 1`以后, 可以复用处于TIME_WAIT的socket为新的连接所用

**tcp_twreuse功能只能是客户端(连接发起方), 因为开启了这个功能,  在调用connect()函数的时候, 内核会随机找出来一个time_wait状态超过1s的连接给新的连接复用**

开启这个功能的一个前提是, 需要打开TCP对时间戳的支持

`net.ipv4.tcp_timestamps = 1(默认为1)`
引入了时间戳, 就不需要通过等待2MSL来确保历史数据已经消失在网络中, 因为时间戳过期的数据包自然会被丢弃

*方式二: net.ipv4.tcp_max_tw_buckets*

这个值默认是18000. **当系统中处于TIME_WAIT的连接一旦超过这个值, 系统无法为这些新关闭的连接分配TIME_WAIT状态的资源, 会直接关闭资源, 也就是向这些连接发送RST包而不是FIN包**

*方式三: 程序中使用SO_LINGER*

通过设置socket的选项, 来设置调用close关闭连接行为

```c
struct linger so_linger;
so_linger.l_onoff = 1;
so_linger.l_linger = 0;
setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger, sizeof(so_linger));
```

如果`l_onoff`为非0, 且`l_linger`的值为0, 调用`close`以后, 就会立刻发送一个`RST`报文给对端, 该TCP连接将跳过第四次挥手

上面的三种方法其实都是在想办法跳过TIME_WAIT状态, 会发生乱七八糟的事情的, 毕竟它设计出来就是为了给下一个TCP连接一个纯净的环境

**如果服务端要避免过多的TIME_WAIT状态的连接, 就永远不要主动断开连接, 让客户端去断开, 由分布在各处的用户端来承受TIME_WAIT**

### 服务器上出现大量的TIME_WAIT状态的原因有哪些

出现大量的TIME_WAIT, 说明TCP连接被频繁关闭, 服务端主动断开了连接, 原因有三

- 没有开启HTTP长连接
- HTTP长连接超时
- HTTP长连接中的请求数量超过最大值

*场景一 : 没有开启HTTP长连接*

客户端和服务端有任意一端发出的HTTP消息头部中的`Connection: close`, HTTP就不会使用长连接, 也就是每次TCP的连接都是走三次握手, 发送消息, 接收响应, 四次挥手这个流程

无论是客户端还是服务端中的HTTP消息头部是`Connection: close`, 都会是**服务端主动断开连接**

解决方法也很简单, 让客户端和服务端都开启长连接就行了

*场景二: HTTP连接超时*

为了防止长连接一直占用系统资源, 长连接会有超时时间, 如果长连接超过超时时间都没有发起新的请求, 定时器的时间一道, 长连接就会被关闭

这个时候解决方法主要是排查网络问题, 看什么原因导致客户端的请求服务端一直没有收到

*场景三: HTTP长连接中的请求数量超过最大值*

Web服务上一般会有个参数来定义一条HTTP长连接上最大能处理的连接数量, 当超过最大限制的时候, 就会主动关闭连接

解决方法就是调高keepalive_requests参数就行

### 服务器出现大量的CLOSE_WAIT状态的原因有哪些

**服务端出现大量CLOSE_WAIT状态的连接的时候, 说明服务端的程序没有调用close函数关闭连接**

一个普通的TCP服务端的流程

1. 创建服务端socket, bind绑定端口, listen端口
2. 将服务端socket注册到epoll
3. epoll_wait等待连接到来, 连接到来的时候, 调用accept 从accept队列中获取已连接的socket
4. 将已连接的socket注册到epoll
5. epoll_wait等待时间发生
6. 对方连接关闭的时候, 调用close

**原因一** : 第二步没有做, 没有将socket注册到epoll, 这样有新连接到来的时候, 服务端就没有办法感知这个事件, 也就无法获取到已经连接的socket, 那么也就没有办法对这个socket调用close了

**原因二: ** 第三步没有做, 当新的连接到来的时候, 没有主动调用accept()获取到该连接的socket, 导致当有大量的客户端断开连接的时候, 服务端无法对该socket调用close()函数关闭连接

**原因三: ** 第四步没有做, 通过accept获取到已经连接的socket以后, 没有注册到epoll, 导致后续收到FIN报文的时候, 没有办法感知这个事件

**原因四: ** 在客户端关闭连接以后, 服务端因为代码逻辑问题, 没有执行close()函数

**服务端出现大量的CLOSE_WAIT状态的连接的时候, 通常都是代码出现了问题导致服务端没办法调用close, 这个时候需要顺着一个TCP服务建立的过程逐步排查**

### 如果已经建立了连接, 但是客户端突然出现故障了怎么办?

这里的情景是客户端断开了连接, 但是同时服务端也一直不给服务端发送请求, 这个时候服务端就永远无法感知到客户端宕机这个事件, 也就是服务端的TCP会一直处于`ESTABLISH`状态

为了避免这个情况, TCP有对应的**保活机制**

定义一个时间段, 如果这个时间段内都没有任何连接相关的活动, TCP保活机制就会开始作用, 每隔一个时间段, 发送一个探测报文, 如果连续几个探测报文都没有得到响应, 则认为当前的TCP连接已经死亡了, 系统内科将错误信息通知给上层应用程序

```shell
net.ipv4.tcp_keepalive_time=7200 // 保活时间, 如果超过这个时间TCP没有任何的连接相关活动就会触发保活机制
net.ipv4.tcp_keepalive_intvl=75  // 每次检测间隔75s
net.ipv4.tcp_keepalive_probes=9 // 9次无响应, 认为对方是不可达的, 中断本次连接
```

也就是在Linux系统中至少需要 7200 + 75*9 = 7875秒 (2h11min15sec)才能发现一个死亡连接

如果应用程序想使用TCP保活机制, 需要通过socket接口设置`SO_KEEPALIVE`选项才能够生效

TCP的保活机制比较长, 我们其实也可以基于这个方式自己在Web服务中实现一个心跳机制

### 如果已经建立了连接, 但是服务端的进程崩溃了会发生什么

TCP的连接信息是由内核维护的, 所以当进程崩溃了以后, 内核需要回收该进程的所有TCP连接资源, 于是内核会发送第一次FIN挥手报文, 后续的挥手过程也是在内核中完成, 并不需要该进程参与
