# CPU Cache

> CPU Cache指的是靠近cpu的SRAM芯片, 电路简单, 访问速度非常快, 同时靠近CPU, 读取速度远快于主存(内存DRAM)

- CPU Cache的出现主要是为了解决CPU和主存之间的速度不匹配这一问题

- 解决上面所述的 "内存墙" 问题, 最核心的思路就是减少对内存的访问次数, 这也是CPU Cache生效的关键
##  CPU Cache是怎么减少内存的访问次数

> 在有了CPU Cache以后, 访问数据会按照  : L1 缓存 -> L2 缓存 -> L3 缓存 -> 主存 -> 硬盘的顺序, 只有当前层没有, 我们才会访问下一层, 直到获取到我们需要的数据

- 我们要减少主存的访问次数, 也就是保证我们能在CPU高速缓存中获取到我们需要的数据
    - 局部性原理
        - **时间局部性** : 最近访问过的数据很可能会被再次访问
        - **空间局部性** : 访问过某个数据以后, 其附近的数据也很可能会被访问
        -  Cache每次读取都会将整个数据块(Cache Line)一次性加载, 利用空间局部性原理
    - 预读技术
        - 根据访问模式, 预测将要使用的数据, 提前将数据从主存中加载到Cache中

## CPU 的缓存一致性

> CPU的缓存一致性, 就是需要保证CPU Cache中的数据和主存的中的数据是一致的(保证最终一致性), 允许短时间的不一致, 但是很明显当其他CPU读取到主存中的该数据的时候, 我们需要保证其他CPU读到的数据是Cache中的或许已经被修改后的数据

- 这个问题换个说法 : **我们什么时候该将数据从Cache中写回主存**

### 写直达

这个想法也是最直观的, 我们同时将数据写入CPU高速缓存和主存, 具体的写入过程是
-  CPU写入数据, 这个数据是否在CPU Cache中
    - 不在 -> 直接写入到主存中
    - 在 -> 先写入到CPU Cache中, 再写入到CPU的主存中
    
> 这个方法的局限性也明显, 不管这个数据在不在CPU高速缓存中, 只要CPU写入了数据, 都需要将数据写回主存中一次


### 写回

既然写直达每次写回操作都会将数据写回到主存中, 为了减少数据写回内存中的频率就出现了写回方法

在写回机制中, 当写操作发生的时候, 新的数据只会被写入到cpu高速缓存中, 只有当修改过的Cache Block 被替换的时候, 才会触发将数据写回到内存中的操作, 减少了数据写回内存的频率

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png)

> 这个方法一言以蔽之, 其实就是在缓存不命中的时候, 才会将被替换的缓存重新写入到主存中

### 多核下的缓存一致性问题

问题的发生
- 加入我们现在有个变量i = 0, 我们A核心和B核心同时运行两个线程, 分别做i++
- 按照我们的预期, 最后的结果应该是2
- 但是会发生, A核心给自己的缓存中的i + 1 变成1, B核心给自己的缓存中的仍为0的i + 1变成1

#### 如何解决缓存一致性问题

只要保证以下两点就能做到缓存一致性
- 写传播 : 某个CPU核心中的Cache数据更新的时候, 必须要传播到其他的核心中的Cache
- 事务串行化 : 串行化也就是和并行化对立, 我们需要让操作按照顺序一一进行, 而不是同步进行, 不然就会发生
    - 核心A想将i = 10, 核心B想将i = 20, 如果不能保证事务的串行化, 我们会不知道最后i会等于多少, 因为是并行发生的(事实上是串行发生, 但是这里的并行会让我们不知道这两个操作发生的先后顺序)

#### 总线嗅探

总线嗅探就是写传播的最常见实现方式, 当CPU A修改了L1Cache中的i的值的时候, 会通过总线将这个事件广播通知其他的所有的核心, 每个CPU上的核心都会监听总线上的广播事件, 并检查自己有没有相同的数据在自己的L1 Cache中, 如果有, 就会将该数据也更新到自己的L1Cache中

> 其他核心将数据更新到自己的Cache上的过程 : 这个过程需要看写传播协议的具体实现, 有写无效协议(MESI协议就是其中之一), 写更新协议
> - 写无效协议 : 会通过总线传播一个让其他CPU将自己缓存中的数据标记的为无效数据, 在下一次访问无效数据的时候, 会发起读请求
> - 写更新协议 : 这个协议不同于写无效协议的按需更新, 而是推式更新, 在CPU-A发情修改以后, 会通过总线将新数据推送至其他CPU


#### MESI

MESI是基于总线嗅探机制实现了CPU缓存一致性的协议, 名字是四个单词的首字符缩写
- Modified, 已修改, 缓存行被修改, 与主存不一致, 只能存在于当前处理器缓存中
- Exclusive, 独占, 缓存行未被修改, 与主存一致, 只存在于当前处理器缓存中 
- Shared, 共享, 缓存行未被修改, 与主存一致, 可能存在于多个处理器缓存中
- Invalidated, 缓存行失效, 不能使用

MESI实现写传播机制, 通过写无效协议的方式
实现事务串行化通过独占数据, 在执行写操作之前, 获取该缓存行的排他所有权 (E或M状态)

| 当前状态 | 事件         | 行为                                                         |
| -------- | ------------ | ------------------------------------------------------------ |
| I        | Local Read   | 1. 如果其他CPU中没有这个数据, 则从主存中读取这个数据, 并将数据标记为E<br />2. 如果其他CPU中有这个数据且为M, 则将数据更新到内存中, 本地核心的缓存再从内存中读取, 并将其他CPU和自己的Cache Line都标记为S<br />3. 如果其他核心的Cache中有这个数据, 且状态为S或E, 本地核心的Cache从缓存中读取这个数据并将这些CPU的Cache都标记为S |
| I        | Local Write  | 如果其他CPU Cache中有这个数据并且为M, 则将数据先写回内存中, 再将其他的CPU中的Cache Line标记为I, 从内存中读取这个数据, 再在缓存中更新这个数据, 并将数据标记为M, 如果其他的CPU |
| I        | Remote Read  | 状态不变                                                     |
| I        | Remote Write | 状态不变                                                     |
| E        | Local Read   | 状态不变                                                     |
| E        | Local Write  | 修改Cache中的数据, 状态变为M                                 |
| E        | Remote Read  | 状态变为S                                                    |
| E        | Remote Write | 将数据标记为I                                                |
| M        | Local Read   | 状态不变                                                     |
| M        | Local Write  | 修改Cache中的数据, 状态不变                                  |
| M        | Remote Read  | 将数据写回内存中, 并将状态变为S                              |
| M        | Remote Write | 将数据写回到内存中, 并将数据标记为I                          |
| S        | Local Read   | 状态不变                                                     |
| S        | Local Write  | 如果其他CPU Cache中有这个数据, 则将数据都标记为I, 将本地的 Cache中的数据修改并标记为M |
| S        | Remote Read  | 状态不变                                                     |
| S        | Remote Write | 将数据标记为I                                                |
|          |              |                                                              |
|          |              |                                                              |
