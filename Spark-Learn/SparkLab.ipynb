{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3193268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/11 12:01:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e26e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsSchema = (\"id integer, name string, price float\")\n",
    "ordersSchema=(\"id integer, itemid integer, count integer\")\n",
    "items = spark.createDataFrame([[0, \"Tomato\", 2.0], \\\n",
    "                            [1, \"Watermelon\", 5.5], \\\n",
    "                            [2, \"pineapple\", 7.0]], \\\n",
    "                            schema=itemsSchema)\n",
    "orders = spark.createDataFrame([[100, 0, 1], \\\n",
    "                            [100, 1, 1], \\\n",
    "                            [101, 2, 3], \\\n",
    "                            [102,2,8]],\\\n",
    "                            schema=ordersSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395236c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "y=(items.join(orders,items.id==orders.itemid, how=\"inner\"))\\\n",
    "        .where(items.id==2)\\\n",
    "        .groupBy(\"name\",\"price\").agg(sum(\"count\")\\\n",
    "        .alias(\"c\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0a451ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['name, 'price], ['name, 'price, 'sum('count) AS c#150]\n",
      "+- Filter (id#136 = 2)\n",
      "   +- Join Inner, (id#136 = itemid#140)\n",
      "      :- LogicalRDD [id#136, name#137, price#138], false\n",
      "      +- LogicalRDD [id#139, itemid#140, count#141], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, price: float, c: bigint\n",
      "Aggregate [name#137, price#138], [name#137, price#138, sum(count#141) AS c#150L]\n",
      "+- Filter (id#136 = 2)\n",
      "   +- Join Inner, (id#136 = itemid#140)\n",
      "      :- LogicalRDD [id#136, name#137, price#138], false\n",
      "      +- LogicalRDD [id#139, itemid#140, count#141], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [name#137, price#138], [name#137, price#138, sum(count#141) AS c#150L]\n",
      "+- Project [name#137, price#138, count#141]\n",
      "   +- Join Inner, (id#136 = itemid#140)\n",
      "      :- Filter (isnotnull(id#136) AND (id#136 = 2))\n",
      "      :  +- LogicalRDD [id#136, name#137, price#138], false\n",
      "      +- Project [itemid#140, count#141]\n",
      "         +- Filter ((itemid#140 = 2) AND isnotnull(itemid#140))\n",
      "            +- LogicalRDD [id#139, itemid#140, count#141], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[name#137, price#138], functions=[sum(count#141)], output=[name#137, price#138, c#150L])\n",
      "   +- Exchange hashpartitioning(name#137, price#138, 200), ENSURE_REQUIREMENTS, [plan_id=423]\n",
      "      +- HashAggregate(keys=[name#137, knownfloatingpointnormalized(normalizenanandzero(price#138)) AS price#138], functions=[partial_sum(count#141)], output=[name#137, price#138, sum#159L])\n",
      "         +- Project [name#137, price#138, count#141]\n",
      "            +- SortMergeJoin [id#136], [itemid#140], Inner\n",
      "               :- Sort [id#136 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#136, 200), ENSURE_REQUIREMENTS, [plan_id=415]\n",
      "               :     +- Filter (isnotnull(id#136) AND (id#136 = 2))\n",
      "               :        +- Scan ExistingRDD[id#136,name#137,price#138]\n",
      "               +- Sort [itemid#140 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(itemid#140, 200), ENSURE_REQUIREMENTS, [plan_id=416]\n",
      "                     +- Project [itemid#140, count#141]\n",
      "                        +- Filter ((itemid#140 = 2) AND isnotnull(itemid#140))\n",
      "                           +- Scan ExistingRDD[id#139,itemid#140,count#141]\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (15)\n",
      "+- HashAggregate (14)\n",
      "   +- Exchange (13)\n",
      "      +- HashAggregate (12)\n",
      "         +- Project (11)\n",
      "            +- SortMergeJoin Inner (10)\n",
      "               :- Sort (4)\n",
      "               :  +- Exchange (3)\n",
      "               :     +- Filter (2)\n",
      "               :        +- Scan ExistingRDD (1)\n",
      "               +- Sort (9)\n",
      "                  +- Exchange (8)\n",
      "                     +- Project (7)\n",
      "                        +- Filter (6)\n",
      "                           +- Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [3]: [id#136, name#137, price#138]\n",
      "Arguments: [id#136, name#137, price#138], MapPartitionsRDD[67] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [id#136, name#137, price#138]\n",
      "Condition : (isnotnull(id#136) AND (id#136 = 2))\n",
      "\n",
      "(3) Exchange\n",
      "Input [3]: [id#136, name#137, price#138]\n",
      "Arguments: hashpartitioning(id#136, 200), ENSURE_REQUIREMENTS, [plan_id=415]\n",
      "\n",
      "(4) Sort\n",
      "Input [3]: [id#136, name#137, price#138]\n",
      "Arguments: [id#136 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD\n",
      "Output [3]: [id#139, itemid#140, count#141]\n",
      "Arguments: [id#139, itemid#140, count#141], MapPartitionsRDD[72] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Filter\n",
      "Input [3]: [id#139, itemid#140, count#141]\n",
      "Condition : ((itemid#140 = 2) AND isnotnull(itemid#140))\n",
      "\n",
      "(7) Project\n",
      "Output [2]: [itemid#140, count#141]\n",
      "Input [3]: [id#139, itemid#140, count#141]\n",
      "\n",
      "(8) Exchange\n",
      "Input [2]: [itemid#140, count#141]\n",
      "Arguments: hashpartitioning(itemid#140, 200), ENSURE_REQUIREMENTS, [plan_id=416]\n",
      "\n",
      "(9) Sort\n",
      "Input [2]: [itemid#140, count#141]\n",
      "Arguments: [itemid#140 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(10) SortMergeJoin\n",
      "Left keys [1]: [id#136]\n",
      "Right keys [1]: [itemid#140]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [3]: [name#137, price#138, count#141]\n",
      "Input [5]: [id#136, name#137, price#138, itemid#140, count#141]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [3]: [name#137, price#138, count#141]\n",
      "Keys [2]: [name#137, knownfloatingpointnormalized(normalizenanandzero(price#138)) AS price#138]\n",
      "Functions [1]: [partial_sum(count#141)]\n",
      "Aggregate Attributes [1]: [sum#158L]\n",
      "Results [3]: [name#137, price#138, sum#159L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [3]: [name#137, price#138, sum#159L]\n",
      "Arguments: hashpartitioning(name#137, price#138, 200), ENSURE_REQUIREMENTS, [plan_id=423]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [3]: [name#137, price#138, sum#159L]\n",
      "Keys [2]: [name#137, price#138]\n",
      "Functions [1]: [sum(count#141)]\n",
      "Aggregate Attributes [1]: [sum(count#141)#157L]\n",
      "Results [3]: [name#137, price#138, sum(count#141)#157L AS c#150L]\n",
      "\n",
      "(15) AdaptiveSparkPlan\n",
      "Output [3]: [name#137, price#138, c#150L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Found 0 WholeStageCodegen subtrees.\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [name#137, price#138], [name#137, price#138, sum(count#141) AS c#150L], Statistics(sizeInBytes=6.19E+37 B)\n",
      "+- Project [name#137, price#138, count#141], Statistics(sizeInBytes=5.57E+37 B)\n",
      "   +- Join Inner, (id#136 = itemid#140), Statistics(sizeInBytes=6.81E+37 B)\n",
      "      :- Filter (isnotnull(id#136) AND (id#136 = 2)), Statistics(sizeInBytes=8.0 EiB)\n",
      "      :  +- LogicalRDD [id#136, name#137, price#138], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "      +- Project [itemid#140, count#141], Statistics(sizeInBytes=6.4 EiB)\n",
      "         +- Filter ((itemid#140 = 2) AND isnotnull(itemid#140)), Statistics(sizeInBytes=8.0 EiB)\n",
      "            +- LogicalRDD [id#139, itemid#140, count#141], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[name#137, price#138], functions=[sum(count#141)], output=[name#137, price#138, c#150L])\n",
      "   +- Exchange hashpartitioning(name#137, price#138, 200), ENSURE_REQUIREMENTS, [plan_id=423]\n",
      "      +- HashAggregate(keys=[name#137, knownfloatingpointnormalized(normalizenanandzero(price#138)) AS price#138], functions=[partial_sum(count#141)], output=[name#137, price#138, sum#159L])\n",
      "         +- Project [name#137, price#138, count#141]\n",
      "            +- SortMergeJoin [id#136], [itemid#140], Inner\n",
      "               :- Sort [id#136 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#136, 200), ENSURE_REQUIREMENTS, [plan_id=415]\n",
      "               :     +- Filter (isnotnull(id#136) AND (id#136 = 2))\n",
      "               :        +- Scan ExistingRDD[id#136,name#137,price#138]\n",
      "               +- Sort [itemid#140 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(itemid#140, 200), ENSURE_REQUIREMENTS, [plan_id=416]\n",
      "                     +- Project [itemid#140, count#141]\n",
      "                        +- Filter ((itemid#140 = 2) AND isnotnull(itemid#140))\n",
      "                           +- Scan ExistingRDD[id#139,itemid#140,count#141]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.explain(mode=\"extended\")\n",
    "\n",
    "y.explain(mode=\"formatted\")\n",
    "\n",
    "y.explain(mode=\"codegen\")\n",
    "\n",
    "y.explain(mode=\"cost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
